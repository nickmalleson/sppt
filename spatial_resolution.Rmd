---
title: "Identifying the Appropriate Spatial Resolution for the Analysis of Crime Patterns"
author: "Nick Malleson"
date: '`r format(Sys.time(), "%d %B, %Y (%H:%M)")`'
output:
  html_document: 
    toc: yes
    pandoc_args: [
      "+RTS", "-K64m",
      "-RTS"
    ]
  pdf_document:
    fig_crop: no
    highlight: kate
    keep_tex: yes
    latex_engine: xelatex
    number_sections: yes
fontsize: 10pt
---

_Note: This was originally part of: the [spatialtest](https://github.com/nickmalleson/spatialtest/) github repository. This file superseeds that one as the code has been tidied up etc._

# Overview 

**Research question**: What is the 'spatial resolution' of different types of crime?

(Or: what is the largest possible spatial resolution that we can use, before aggregation hides important lower-level variation)

(Or: is crime more/less concentrated than other phenomena?)

(Or: repeat the 2011 crime stability paper?)

We know that for many types of crime if you aggregate data to anything larger than a block then you hide important underlying spatial variation, but is this resolution the same for all types of crime? At what point are the diminishing returns of increased spatial resolution take over?

**Method**

 - For a number of different of crime types (point patterns), choose two time periods (_can refer to Martin's temporal sppt paper to identify crimes and time periods that are stable_). Then:
 
 1. Aggregate the points to a regular grid
 2. Calculate the S-Index (local and global)
 3. Shift the grids multiple times in N, E, S, W in order to reduce the impact of MAUP and calculate S-Index again
 4. Increase the resolution of the grid, and repeat


**Results**

 - Graph the resolution against S-Index for the different crimes (see below)
 - At the point where the graph ‘kinks’ you have found the optimal resolution (cells as large as they can go before the differences start to explode).
 - (Fuzzy) map of the local S-Index to see where there are differences (i.e. maybe it is more appropriate to change the resolution by area)
 - Could also explore the relationship in more detail, e.g. log plots become linear for some crimes? What impact does this have on how they should be analysed? Etc.

**Decisions / for discussion**

 - Need to decide on temporal resolution for comparison
   - Need two patterns for which we expect no structural change – i.e. any differences are random. 



## Notes on the Multiscale Spatial Error Assessment Method

This is the R implementation and further development of the Multiscale Spatial Error Assessment method (aka [Multiscale Validation](http://www.geog.leeds.ac.uk/courses/other/programming/practicals/general/modelling/validation/multiscale-code/2.html)).

The aim of the method is to take two point-patterns, iteratively aggregate the points to cells of increasing size, and calculate the error between the two data sets at the different grid resoultions. See the right images for an example (from  [here](http://www.geog.leeds.ac.uk/courses/other/programming/practicals/general/modelling/validation/multiscale-code/2.html)).

The method has has been used in the following paper:

Malleson, N., A. Heppenstall, L. See, A. Evans (2013) Using an agent-based crime simulation to predict the effects of urban regeneration on individual household burglary risk. _Environment and Planning B: Planning and Design._ 40(3) 405-426. Available [here](http://www.envplan.com/abstract.cgi?id=b38057) and [here](http://nickmalleson.co.uk/wp-content/uploads/2013/05/EPB-V6-forBlog.pdf) (if you don't have access to the journal)

Thanks to Lex Comber and Chris Brunsdon for their excellent book 'R for Spatial Analysis and Mapping'. I got most of the R GIS stuff from there.

Brunsdon, C and Comber, L (2015) _An Introduction to R for Spatial Anaysis and Mapping_. Sage

<img style="float:right; width:40%"
src="http://www.geog.leeds.ac.uk/courses/other/programming/practicals/general/modelling/validation/multiscale-code/images/graph.jpg"
/>

## Configuration and library loading

Configure the script here. (Un)comment or edit lines as appropriate to change the data used and the parameters for the method. (_Note: R code not included in output_).

```{r initialise, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
WORKING_DIR <- '~/mapping/projects/sppt/'
setwd(WORKING_DIR)

library(GISTools)
#library(rgeos)    # For things like gIntersects
library(rgdal)     # For reading shapefiles
library(raster)    # For creating regular grids
#library(plyr)     # For counting and aggregating
#library(tmap)     # For thematic maps
library(classInt) # Jenks natural breaks
#library(png)      # For loading pngs after they have been written
#library(grid)     # so that they can be embedded in knitted documents
#library(spdep)    # For doing the spatial regression, contiguity matrices, etc.
#library(GWmodel)  # For geographically weighted regression (GWR)
#library(MASS)     # For stepwise regression (stepAIC())
#library(pander)    # For printing tables nicely
#library(MVN)      # For testing for multivariate normality (MVN)
#library(RColorBrewer) # For making nice colour themes
library(hydroGOF)   # Has an rmse() function
library(xtable)   # For making latex/html tables
library(parallel) # For ruonning things in parallel (e.g. mclapply())
no_cores <- detectCores() / 4 # Detect the number of cores that are available and use half (often CPUs simulate 2 threads per core)
Sys.setenv(MC_CORES=no_cores) # Run on n cores (I'm not sure which of these
options("mc.cores"=no_cores) # is correct).
library(Deriv)   # For calculating deriviatives

library(sppt) # The spatial point pattern test library (see https://github.com/wsteenbeek/sppt for install instructions)

```
 
# Data

We use publicly-available Vancouver crime data, available [here](http://data.vancouver.ca/datacatalogue/crime-data.htm).

Data are available from: [http://data.vancouver.ca/datacatalogue/crime-data-details.htm](http://data.vancouver.ca/datacatalogue/crime-data-details.htm) and there is a single zip file: [ftp://webftp.vancouver.ca/opendata/shape/crime_shp_all_years.zip](ftp://webftp.vancouver.ca/opendata/shape/crime_shp_all_years.zip).

The script begins by downloading the data (if necessary).

```{r downloadData }
ZIPFILENAME <- "./data-raw/vancouver_public/crime_shp_all_years.zip"

if (!file.exists(ZIPFILENAME)) {
  print("Downloading crime data")
  download.file(url = "ftp://webftp.vancouver.ca/opendata/shape/crime_shp_all_years.zip", destfile = ZIPFILENAME)
}

```

Then read it in:

```{r readData, cache=TRUE}

# Unzip the files into the working directory
zipfile <- unzip(ZIPFILENAME)

# Read the shapefile
all.crime <- readOGR(dsn="./crime_shp_all_years", layer = "crime_shp_all_years")

# Delete the extracted file
unlink(zipfile)
unlink("crime_shp_all_years", recursive = TRUE) # a left over directory

# Assign short codes to crime types (the below throws an error about 'values must be length 1'
# if there are any crime types that aren't matched to short equivalents)
all.crime$TYPE2 <- as.factor(unlist(
  mclapply(X = all.crime$TYPE, FUN = function(t) {
    t2 <- switch(as.character(t),
          "Break and Enter Commercial" = "BNEC", 
          "Break and Enter Residential/Other" = "BNER",
          "Mischief" = "MISCHIEF",
          "Other Theft" = "OTHERTHEFT",
          "Theft from Vehicle" = "TFV",
          "Theft of Bicycle" = "TOB",
          "Theft of Vehicle" = "TOV",
          "Vehicle Collision or Pedestrian Struck (with Fatality)" = "COLFAT", 
          "Vehicle Collision or Pedestrian Struck (with Injury)" = "COLINJ")
})))
  
# Drop 2018 (not sufficient data yet)
all.crime <- all.crime[which(all.crime$YEAR<2018),]

rm(ZIPFILENAME)
```

How many different crimes are there, by year:

```{r allCrimeByYear-table, results="asis" }
print(xtable(table(all.crime$TYPE, all.crime$YEAR)), type="html")
```

Do a graph as well, indexed to volume at the start

```{r allCrimeByYear-graph, fig.width=9, fig.height=6 }

crime.table <- table(all.crime$TYPE, all.crime$YEAR)
crime.types <- rownames(crime.table)
crime.years <- sapply(X = colnames(crime.table), FUN = strtoi)

for ( i in 1:length(crime.types)) {
  type <- crime.types[i]
  
  # Need to index
  start <- crime.table[type,1]
  yval <- 100 * ( crime.table[type,] / start)
  if (i==1) {
    plot(x=crime.years, y=yval, type='o', lty=1, pch=i, col=i, ylim=c(0,250),
         main="Change in crime volumes over time", ylab="Number of crimes (indexed)", xlab="Year"
         )
  }
  lines(x=crime.years, y=yval, type='o', lty=1, pch=i, col=i)
  legend("topleft", legend = crime.types, col=1:length(crime.types), lty=1, pch=1:length(crime.types), cex=0.7)
}
rm(start, yval, type)
```

For context, see how many there are in total (using short names of crimes)

```{r allCrime-totals_graph, fig.width=7, fig.height=4 }
barplot(rowSums(table(all.crime$TYPE2, all.crime$YEAR)
), horiz=TRUE, cex.names=0.8, las=1)
```

Split the big file up into different crimes and time periods

```{r subsetRawData, cache=TRUE}
# A directory for the crime data (in case it doesn't exist)
dir.create(file.path("./ROUT", "crime"), showWarnings = FALSE)

# Shorter versions of the crime types
crime.types2 <- unique(all.crime$TYPE2)

for (type in crime.types2) {
  assign(as.character(type), all.crime[all.crime$TYPE2==type,])
  # Write them as well as they can be useful later
  writeOGR(all.crime[all.crime$TYPE2==type,], dsn = "./ROUT/crime", layer = type, 
           driver = "ESRI Shapefile", overwrite_layer = TRUE)
}

```

# The Multi-Scale Error Assessment Method (MSEA)

The method works as follows:

 1. Aggregate the points to a regular grid
 2. Calculate the S-Index (local and global) (as well as some other statistics)
 3. Shift the grids multiple times in N, E, S, W in order to reduce the impact of MAUP and calculate S-Index again
 4. Increase the resolution of the grid, and repeat
  
The following defines a function that will run the method (_the R code has been hidden from the output because it is so long_).

```{r defineMSEA, include=FALSE}

#' Multi-Scale Error Assessment (MSEA).
#'
#' Run the Multi-Scale Error Assessment (MSEA) method, returning the difference between two point data sets at different geographical scales using different measures of error
#' 
#' XXXX OTHER DETAILS (i.e. how the method works?)
#' 
#' @param points1 A set of points (a SpatialPointsDataFrame, or SpatialPoints object)
#' @param points2 The set of points to compare against (another SpatialPointsDataFrame or SpatialPoints object)
#' @param N The number of times to sub-divide the largest cell. I.e. if N=20 then the smallest grid at which 
#'    error is calculated will be 20*20 cells. Note: the algorithm starts at i=2 which gives 2*2=4 cells as 
#'    the most coarse resolution (doesn't make sense to calculate error for 1 data point)
#' @param n.shifts The number of times to shift the grid at each resolution
#' @param ignore.zeros Whether to remove cells with no crimes in them (the abundance of zeros at higher resolutions can distort the statistics)
#' @param step The number of steps in between two resolutions before re-calculating the grid. Default 1, i.e. 
#'    every sub-division between 1 and N will be calculated. If \code{step} were \code{2}, then only every
#'    other grid resolution will actually be computed.
#' @param return.sobject Whether to return the object that is created by sppt. Default true. Setting
#'    to false can reduce the size of the results
#' @param return.grids Whether to return the actual grids (spatial objects).  Default true. Setting
#'    to false can reduce the size of the results
#'
#' @return An list that encapsulates the results of the test as well as some other useful information:
#' 
#'  \code{results} - a list of the individual grids computed for each cell size. This will have length N. Each item in the list is a SpatialPolygonsDataFrame with the following attributes:
#'  \itemize{
#'  \item{"CellID"}{A unique numerical ID for each cell in the grid}
#'  \item{"points1"}{The number of points from the points1 dataset that fall within the cell}
#'  \item{"points2"}{The number of points from the points2 dataset that fall within the cell}
#'  \item{"p1.pct"}{The percentage of points from the points1 dataset that fall within the cell}
#'  \item{"p2.pct"}{The percentage of points from the points1 dataset that fall within the cell}
#'  \item{"diff"}{The difference in the number of points (points1 - points2)}
#'  \item{"abs.diff"}{The absolute difference in the number of points}
#'  \item{"abs.pct.diff"}{The absolute difference in the percentages (nice to map)}
#'  \item{"localS"}{Andresen's Local S Index}
#'  \item{"localS.robust"}{Robust version of Andresen's Local S Index (ignores areas with no points)}
#'  \item{"similarity"}{The similarity (like S but just highlights difference, not direction)}
#'  \item{"similarity.robust"}{Robust version of \code{similarity}}
#'  \item{"ConfLowP"}{The lower confidence limit for the S Index}
#'  \item{"ConfUppP"}{The upper confidence limit for the S Index}
#' }
#'   Note that if \code{return.grids} is false then this isn't returned
#'
#'  \code{cell.areas} - A vector (length N) with the square area of a cell in each grid
#'
#'  \code{num.cells} - A vector (length N) with the number of cells in each grid
#'
#'  \code{rss} - a vector of length N with the residual sum of squares error value for each grid
#'
#'  \code{r.squared} - a vector of length N with the R-Squared error for each grid
#'
#'  \code{rmse} - a vector of length N with the Root Mean Square Error value for each grid
#'
#'  \code{globalS} - a vector of length N with Andresen's Global S index
#'
#'  \code{globalS.robust} - a vector of length N with the robust version of Andresen's Global S index that 
#'    ignores areas with no points (note that if \code{ignore.zeros} is \code[TRUE] then this is the
#'    same as the \code{globalS})
#' 
#'  \code{iteration} - a vector of length N with the iteration number that this grid is part of
#' 
#'  \code{shift} - a vector of length N with the shift number of this grid (e.g. one of 1--10 if there
#'    are ten shifts per iteration).
#'
#'
#'  \code{s.object} - a vector of length N with the object returned by the call to \code{sppt} (used to 
#'    calculate the S Index). This can be useful for debugging etc, but is probably unnecessary as 
#'    all of the useful global and local information returned from \code{sppt} have been included directly.
#'    Note that if `return.sobject` is FALSE then this isn't returned.
#'
#' @examples
#' rmse(points1, points2)
msea <- function(points1, points2, N=20, n.shifts = 10, ignore.zeros=FALSE, step=1, 
                 return.sobject=TRUE, return.grids=TRUE) {
  
  # Check that the projections are the same
  if ( proj4string(points1) != proj4string(points2) ) {
    warning("The points1 and points2 projections are different, this will probably lead to catastrophic results!")
    stop()
  }
    
  bb <- bbox(points1 + points1) # A bounding box around all points
  
  # Store all grids (data frames) in a big long list
  results <- list()
  # Remember some other things that are useful later
  cell.areas <- c() # The area of the cells
  num.cells <- c()  # The number of cells in each iteration

  # Remember the global errors associated with each grid
  rss <- c() # Residual sum of squares
  r.squared <- c()
  rmse <- c()
  globalS <- c()
  globalS.robust <- c()
  
  # Keep a link to the object that is returned from the call to sppt. Useful for debugging mostly.
  s.object <- c()
  
  # Remember the iteration and shift numbers (these are the i and j in the nested loops)
  iteration <- c()
  shift <- c()
  
  # Create the grids - adapted from Brunsdon & Comber (2015, p150)
  # Note: will actually start at i=2 which gives 2*2=4 cells (doesn't make sense to calculate error for 1 data point)
  # but it's easier to start from i=1 and delete that result afterwards
  counter <- 1 # for counting the total number of grids created (required for indexing)
  for (i in seq(from=1,to=N,by=step)) {
    # Cell size is the total width divided by the number of cells to draw so far (i)
    cell.width <-  (bb[1,2] - bb[1,1]) / i
    cell.height <- (bb[2,2] - bb[2,1]) / i
    
    # Make the bounding box slightly larger than necessary (by half a cell in each direction), 
    # so when the grid is shifted there wont be any points outside
    # It needs to be big enough so that it can have one extra ring of cells around it
    bb.larger <- bb # The new bounding box
    bb.larger["coords.x1","min"] <- bb["coords.x1","min"] - ( cell.width / 2  ) # Min x gets smaller
    bb.larger["coords.x2","min"] <- bb["coords.x2","min"] - ( cell.height / 2 ) # Min y gets smaller
    bb.larger["coords.x1","max"] <- bb["coords.x1","max"] + ( cell.width / 2  ) # Max x gets larger
    bb.larger["coords.x2","max"] <- bb["coords.x2","max"] + ( cell.height / 2 ) # Max y gets larger
    
    # For each resolution, repeat a few times by slightly shifting the grid by a random amount in a random direction
    for (j in 1:n.shifts) {
      # Remember the cell area (useful later) (needs to be repeated for each shift)
      cell.areas <- c(cell.areas, (cell.width * cell.height) )

      # Chose random N-S and E-W directions to shift the grid in (using a random uniform distribution)
      shift.x <- runif(n=1, min=-cell.width /2, max=cell.width /2 )
      shift.y <- runif(n=1, min=-cell.height/2, max=cell.height/2)
      
      # Calculate the centre of the lower-left cell (the one with the smallest coordinates),
      # taking into account the shift
      centre.x <- bb.larger[1,1] + ( cell.width  / 2 ) + shift.x
      centre.y <- bb.larger[2,1] + ( cell.height / 2 ) + shift.y
      
      # Create a grid  
      grd <- GridTopology(
        cellcentre.offset = c(centre.x, centre.y), # No offset, the grid will just cover all the points
        cellsize = c(cell.width, cell.height),
        cells.dim = c(i+1,i+1)
      )
      
      number.of.cells <- (i+1) * (i+1) # Add an extra row and column to account for shifting 
      num.cells <- c(num.cells, number.of.cells) # Remember the number of cells in this iteration
      
      # Convert the grid into a SpatialPolygonsDataFrame
      spdf <- SpatialPolygonsDataFrame(
        as.SpatialPolygons.GridTopology(grd),
        data = data.frame(c(1:number.of.cells)),
        match.ID = FALSE
      )
      proj4string(spdf) <- proj4string(points1)
      names(spdf) <- "CellID" # Name the column
      
      # Aggregate the points
      spdf@data$points1 <- poly.counts(points1, spdf)
      spdf@data$points2 <- poly.counts(points2, spdf)
      
      # Drop cells with 0 for both counts?
      if (ignore.zeros) {
        spdf <- spdf[which(spdf@data$points1>0 | spdf@data$points2>0),]
        stopifnot( length(which(spdf@data$points1==0 & spdf@data$points2==0)) == 0 )
      }
      
      # Calculate percentages of points in each area (might be useful)
      spdf@data$p1.pct <- 100 * spdf@data$points1 / sum(spdf@data$points1 )
      spdf@data$p2.pct <- 100 * spdf@data$points2 / sum(spdf@data$points2 )
      
      # Calculate the errors 
      
      # Difference in the number of points
      spdf@data$diff <- spdf@data$points1 - spdf@data$points2
      
      # Absolute difference
      spdf@data$abs.diff <- abs(spdf@data$points1 - spdf@data$points2)
      
      # Absolute Difference in percentages
      spdf@data$abs.pct.diff <- abs(spdf@data$p1.pct - spdf@data$p2.pct)
      
      # The Local S Index (slightly more convoluted)
      s <- sppt(base_points.sp = points1, test_points.sp = points2, uoa.sp = spdf) # Calculate the index
      
      # Sanity check - check the sppt package calculates the same percentages as this code
      stopifnot( identical(s$CELLID,spdf$CELLID) ) # Check the cells are in the same order (avoids having to merge on cell ID)
      # XXXX For some reason the two methods aggregat the points very slightly differently. 
      # Uncomment these below, debug, and do something like 'cbind(spdf$points1, s$NumBsePts)' to see the different counts
      #stopifnot(identical(spdf$points1+spdf$points1, s$SumBseTstPts))
      #stopifnot(identical(spdf@data$p1.pct, s$PctBsePts))
      #stopifnot(identical(spdf@data$p2.pct, s$PctTstPts))
      
      # Useful Stats. associated with the S Index 
      # (the whole S object is also returned later too, but these are more convenient to have direct access to)
      spdf@data$localS            <- s@data$localS
      spdf@data$localS.robust     <- s@data$localS.robust
      spdf@data$similarity.robust <- s@data$similarity.robust
      spdf@data$ConfLowP          <- s@data$ConfLowP
      spdf@data$ConfUppP          <- s@data$ConfUppP
      spdf@data$similarity        <- s@data$similarity
      
      # Store this result
      # Formula below (from Nikee) wont work because we might step over some iterations in the inner loop
      #grid.num <- ((i-1) * n.shifts) + j # Calculate total number of grids created so far
      #results[[ grid.num ]] <- spdf
      results[[ counter ]] <- spdf
      counter <- counter + 1
      
      # Now calculte the global errors
     
      # RSS 
      rss <- c(rss, sum( ( spdf@data$points1 - spdf@data$points2 )**2 ))
      # R squared
      r.squared <- c(r.squared, summary(lm(spdf@data$points1 ~ spdf@data$points2, data=spdf@data))$r.squared )
      # RMSE
      rmse <- c(rmse, rmse(spdf@data$points1, spdf@data$points2) )
      # Global S Index (normal and robust). In globalS, each area has same value for global S, so take 1st row
      # arbitrarily. For robust version, need to find the first row that isn't NA (hence use min()).
      #. For
      globalS <- c(globalS, s@data[1,"globalS"]        ) 
      globalS.robust <- c(globalS.robust, s@data[min(which(!is.na(s@data$globalS.robust))),"globalS.robust"] )
      
      # Sometimes useful to keep a reference to the raw results returned by the sppt call (mostly for debugging)
      s.object <- c(s.object, s)
      
      # Also useful to know which iteration number and grid shift this is (useful for naming grids)
      iteration <- c(iteration, i)
      shift <- c(shift, j)
      
      
    } # for shifting grids
    
  } # for cell sizes
  
  
  
  # TODO make parallel: I would need a function that used the outer loop index (i.e. each core will run each resolution)
  # and returned a named list, with each of the components below included in the list. 
  # Then the 'return the results' bit below can go through the results list, exrracting each component
  # (an lapply) and combining them into single vectors
  
  
  
  # Delete the results that used one single large cell as these don't mean anything
  results[[1]] <- NULL
  num.cells <-      num.cells     [2:length(num.cells)]
  cell.areas <-     cell.areas    [2:length(cell.areas)]
  rss <-            rss           [2:length(rss)]
  r.squared <-      r.squared     [2:length(r.squared)]
  rmse <-           rmse          [2:length(rmse)]
  globalS <-        globalS       [2:length(globalS)]
  globalS.robust <- globalS.robust[2:length(globalS.robust)]
  s.object <-       s.object      [2:length(s.object)]
  iteration <-      iteration     [2:length(iteration)]
  shift <-          shift         [2:length(shift)]
  
  # Sanity check - global errors and other info should be vectors of the same length
  stopifnot(
    length(num.cells) == length(cell.areas) &
    length(num.cells) == length(rss) &
    length(num.cells) == length(r.squared) &
    length(num.cells) == length(rmse) & 
    length(num.cells) == length(globalS) & 
    length(num.cells) == length(globalS.robust) &
    length(num.cells) == length(s.object) &
    length(num.cells) == length(iteration) &
    length(num.cells) == length(shift)
  )

  # Return the results
  r <- list(
    "results" = if (return.grids) results else NA,
    "cell.areas" =cell.areas,
    "num.cells" = num.cells,
    "rss" = rss,
    "r.squared" = r.squared,
    "rmse" = rmse,
    "globalS" = globalS,
    "globalS.robust" = globalS.robust,
    "s.object" = if (return.sobject) s.object else NA,
    "iteration" = iteration,
    "shift" = shift
  )
  return(r)
  
} # function
  
```

# Results

## Run the MSEA

Note that when zeros are included in the analysis, the similarity of the comparison increases. This is because there will be many cells with no crimes, hence the _same number_ of crimes (0). However, when we examine the graph of resolution against similarity, the shape of the graphs is identical so, importantly, the 1st derivatives are identical. This means that in the later analysis it doesn't matter whether cells with zero counts are included or not as we will always find the same 'elbow'/kink in the graph. This can be verified by running an earlier version of this file that included cells with and without zero:

 - https://github.com/nickmalleson/sppt/blob/2692a3676fa6c58f9041b637b0f649ec4cc0b908/spatial_resolution.Rmd

In the subsequent analysis cells with no crimes will be included.

Run the MSEA algorithm on BNER, BNEC, TFV and TOB, comparing 2015 and 2016.

```{r runMSEA-all, cache=TRUE }

# Put the data into lists so I don't have to do too much repeating

data1 <- list("base" = BNER[BNER$YEAR==2015,], "test"= BNER[BNER$YEAR==2016,], name="BNER")
data2 <- list("base" = BNEC[BNEC$YEAR==2015,], "test"= BNEC[BNEC$YEAR==2016,], name="BNEC")
data3 <- list("base" = TFV[TFV$YEAR==2015,], "test"= TFV[TFV$YEAR==2016,], name="TFV")
data4 <- list("base" = TOB[TOB$YEAR==2015,], "test"= TOB[TOB$YEAR==2016,], name="TOB")
all <- list(data1, data2, data3, data4)
rm(data1, data2, data3, data4)

# Do all crime types at the same time.
tmp <- mclapply(X = all, FUN = function(x) {
  msea(x[['base']], x[['test']], N=150, n.shifts=5, ignore.zeros = F, step=5, 
       return.sobject=FALSE, # Don't return the s.objects, nor the spatial grids
       return.grids=FALSE)   # to save memory
}, mc.preschedule=FALSE)
stopifnot(length(tmp)==4)
# Store the results in the lists in an item called 'r'
for (i in 1:length(tmp)) {
  all[[i]][['r']] <- tmp[[i]]
}
rm(tmp)
gc() # Do some garbage collection after deleting the big temporary objects (maybe not necessary)
save.image('temp.RData')
```

Also run again once for BNER to do the fuzzy mapping. These results wont be used in the graph, so don't need to calculate the index as many times 

(The code above doesn't actually return the spatial results grids because it takes a huge amount of memory (_will work, but needs a PC with > 20Gb memory or so or it becomes unusable_))

```{r runMSEA-bner, cache=TRUE}
p1 <- BNER[BNER$YEAR==2015,]
p2 <- BNER[BNER$YEAR==2016,]
bner.result <- msea(points1 = p1, points2 = p2,
                    N=150, n.shifts=3, ignore.zeros = F, step=10,
                    return.sobject=TRUE,
                    return.grids=TRUE)
save.image('temp.RData')
```

## Map the Local S Index

Map the Local S for a few grids, just out of interest

```{r map.globalS, fig.width=8, fig.height=6 }
statistic <- "localS" # The statistic to map

par(mfrow=c(2,2))

for (i in 4:1) {
  index <- round(length(bner.result$results) / i )
  
  # Create the shading manually
  s <- list("breaks"=c(-0.5, 0, 0.5), "cols"=c("#fc8d59","#ffffbf", "#ffffbf","#99d594"))
  
  choropleth(bner.result$results[[index]], bner.result$results[[index]]@data[,statistic], main=paste(statistic,i),
             lty=0, shading = s)
  points(p1, col='blue', cex=0.5)
  points(p2, col='red',  cex=0.5)
  choro.legend("topright", sh = s, cex=0.5)
}
```

Also do a fuzzy map of the difference in local S values.

```{r map.globalS.fuzzy, fig.width=6, fig.height=5}
# Maximum number of results we can map is 256 because transparency is represented by a two-digit hex
# I actually set this as lower otherwise it doesn't work (all the colours add up to red)
results.to.map <- NA
if (length(bner.result$results) > 50) {
  results.to.map <- sample(x=1:length(bner.result$results), size=50, replace = FALSE )
  print("Only mapping 50 of all grids ")
} else {
  results.to.map <- 1:(length(bner.result$results)-1)
}

par(mfrow=c(1,1))
last.result <- bner.result$results[[length(bner.result$results)]] # Convenience for list result generated (the smallest grid)

# Shading
s.noalpha <- list("breaks"=c(-0.5, 0, 0.5), "cols"=c("#fc8d59","#ffffbf", "#ffffbf","#99d594"))
s         <- list("breaks"=c(-0.5, 0, 0.5), 
                  "cols"=add.alpha(c("#fc8d59","#ffffbf", "#ffffbf","#99d594"), 1/length(results.to.map)))

choropleth(
  last.result, last.result@data[,statistic], shading = s, main=paste0("Fuzzy ",statistic), lty=0
  )

for ( index in results.to.map) {
  choropleth( bner.result$results[[index]], bner.result$results[[index]]@data[,statistic], shading = s, lty=0, add=TRUE )
}

# For the legend, redo the shading as above but without the alpha (don't want the legend colours to be transparent)

choro.legend("topright", sh=s.noalpha, cex=0.8)
```


## Spatial Error Resolution

### All Error Measures

Graph the differences using the number of cells used.

XXXX HERE

GRAPH THE ERRORS, USING THE 4 AVAILABLE STATISTICS, FOR EACH CRIME TYPE


### S Index

Prior research showed that a log model was the most successful at representing the curve of Local S against resolution (measured by number of cells). This can be verified by running an earlier version of this file that included cells with and without zero:

 - https://github.com/nickmalleson/sppt/blob/2692a3676fa6c58f9041b637b0f649ec4cc0b908/spatial_resolution.Rmd

Plot the log model and its derivatives.

```{r s.model-all-log, fig.width=9, fig.height=9 }
options(scipen=1) # A bit of scientific notation is OK
par(mfrow=c(length(all),4))

START <- 10 # Drop those first few really big cells
END   <- length(all[[1]]$r$num.cells)

FORM.LOG <- y ~ log(x) # A formula and function for log(x)

col1 <- "#fc8d59"
col2 <- "#555555"
col3 <- "#99d594"

for (a in all) {
  r <- a$r # This is the results of the msea test
  name <- a$name # This is the name of the crime type
  base <- a$base # Base and test points (not actually needed here)
  test <- a$test
  
  x <- r$num.cells[START:END]
  y <- r$globalS[START:END]
  
  # Plot points
  plot(  x, y, main=name, ylim=c(0,1), col="#999999", pch='o', 
         xlab="Number of Cells", ylab="Global S Index")
  
  # Calculate models for points using FORM
  
  m.log  <- lm( FORM.LOG,  data=list("x"=x, "y"=y) )
  lines(x=x, y=predict(m.log ), col=1, lwd=2)
  
  # Check that they have the extected number of coefficients
  stopifnot(length(coef(m.log ))==2)
  
  # Calculate gradients
  
  # Function to differentiate.
  FUNC.log <- function(x) coef(m.log)[1] + (coef(m.log)[2]*log(x) )

  stopifnot(! (FALSE %in% (predict(m.log ) == FUNC.log (x)))) # Check function has been specified correctly
  
  # Differentiate the functions using three derivatives
  d.log1 <- Deriv( f = FUNC.log,  x="x", nderiv = 1 )
  d.log2 <- Deriv( f = FUNC.log,  x="x", nderiv = 2 )
  d.log3 <- Deriv( f = FUNC.log,  x="x", nderiv = 3 )
  
  grads.log1 <- vapply(X = x, FUN=d.log1, FUN.VALUE = numeric(1))
  grads.log2 <- vapply(X = x, FUN=d.log2, FUN.VALUE = numeric(1))
  grads.log3 <- vapply(X = x, FUN=d.log3, FUN.VALUE = numeric(1))
  
  # Mark the points of the elbow (when the gradient is closest to zero)
  #lines(abline(v=x[which.min(abs(grads.log  - 0))], col=col3, lty=2, lwd=2)) # Point at which the gradient is closest to 0

  # Also plot the gradients as a separate graph
  plot( x=x, y=grads.log1, col=col1, pch=1, ylab="Gradient", main="1st Derivative", ylim=c(0,0.0005))
  abline(v=x[which.min(abs(grads.log1 - 0))], col="blue", lty=2, lwd=2) # Point at which the gradient is closest to 0
  abline(h=0, lty=2, lwd=2) # y=0
  
  plot( x=x, y=grads.log2, col=col2, pch=1, ylab="Gradient", main="2nd Derivative", ylim=c(-0.0000005,0.0000005))
  abline(v=x[which.min(abs(grads.log2 - 0))], col="blue", lty=2, lwd=2) # Point at which the gradient is closest to 0
  abline(h=0, lty=2) # y=0
  
  plot( x=x, y=grads.log3, col=col3, pch=1, ylab="Gradient", main="3rd Derivative", ylim=c(0,0.00000000001))
  abline(v=x[which.min(abs(grads.log2 - 0))], col="blue", lty=2, lwd=2) # Point at which the gradient is closest to 0
  abline(h=0, lty=2) # y=0
  
  
  #lines(abline(v=x[which.min(abs(grads.sqrt - 0))], col=col2, lty=2, lwd=2)) # Point at which the gradient is closest to 0
  #lines(abline(v=x[which.min(abs(grads.log  - 0))], col=col3, lty=2, lwd=2)) # Point at which the gradient is closest to 0
  
  #legend("topright", legend=c("log d1", "log d2", "log d3"), pch="o", col=c(col1,col2,col3))
  
}

```




Do a single graph that we can use for the paper (when explaining how the method works). Use TOB because the model fits the data well (it's only for illustrative purposes so doesn't really matter.)

```{r example_error_graph_for_paper, fig.width=9, fig.height=6 }
options(scipen=1) # A bit of scientific notation is OK
par(mfrow=c(1,1))

START <- 10 # Drop those first few really big cells
END   <- length(all[[1]]$r$num.cells)

FORM.LOG  <- y ~ log(x)

r <- all[[4]]$r # This is the TOB data
stopifnot(all[[4]]$name == "TOB")

x <- r$num.cells[START:END]
y <- r$globalS[START:END]

# Calculate models for points using FORM
m.log  <- lm( FORM.LOG,  data=list("x"=x, "y"=y) )

# Check that the model has the extected number of coefficients
stopifnot(length(coef(m.log ))==2)

# Plot
plot(  x, y, main="Similarity of two point patterns at different resolutions", ylim=c(0.4,1), col="black", axes=FALSE,
       xlab="Number of Cells (Resolution)", ylab="Global S Index (Similarity)")
Axis(side=1, labels=FALSE)
Axis(side=2, labels=FALSE)
lines(x=x, y=predict(m.log ), col="red", lwd=2, lty=2)


# And a pdf for the paper
pdf(file="./figs_for_paper/example_error_graph_for_paper.pdf", width=6, height=8)
plot( x, y, ylim=c(0.4,1), col="black", axes=FALSE, cex.lab = 1.2,
      xlab="Number of Cells (Resolution)", ylab="Global S Index (Similarity)")
Axis(side=1, labels=FALSE)
Axis(side=2, labels=FALSE)
lines(x=x, y=predict(m.log ), col="red", lwd=2, lty=2)
dev.off()
  

```


_Could also do **Lorenze curves** at some of the resolutions (as per Eck's [review of crime concentrations](https://crimesciencejournal.springeropen.com/articles/10.1186/s40163-017-0070-4)).


# Other

See if there were any warnings:

```{r warnings }
warnings()
```

Save the results for mapping or whatever:

XXXX This needs updating because now I don't actually generate the spatial grids when running the anlaysis because they take up too much room. The old version of this file did that:

 - https://github.com/nickmalleson/sppt/blob/2692a3676fa6c58f9041b637b0f649ec4cc0b908/spatial_resolution.Rmd

```{r save.results, warning=FALSE, eval=FALSE }
# All of the R objects. Makes it easy to re-read the results
save.image("spatial_resolution.RData")

# The numbers used to make the graphs
write.csv(
  #data.frame("CellArea"=r$cell.areas, "NumCells"=r$num.cells, "RSS"=r$rss, "RSquared"=r$r.squared, "RMSE"=r$rmse,
  #           "GlobalS" = r$globalS, "GlobalSRobust" = r$globalS.robust), 
  data.frame("CellArea"=r.withzeros$cell.areas, "NumCells"=r.withzeros$num.cells, "RSS"=r.withzeros$rss,
             "RSquared"=r.withzeros$r.squared, "RMSE"=r.withzeros$rmse,
             "GlobalS" = r.withzeros$globalS, "GlobalSRobust" = r.withzeros$globalS.robust), 
  file = "ROUT/results.csv")

# Delete old shapefiles

files <- list.files(path = "./ROUT", pattern = "result_")
for (f in files) {
  file.remove(paste0("./ROUT/",f))
}

# Write new ones, using the i and j counters from the nested loops to name the files
for (x in 1:length(r.withzeros$results)) {
  res <- r.withzeros$results[[x]]
  i <-   r.withzeros$iteration[x]
  j <-   r.withzeros$shift[x]
  writeOGR(obj = res, dsn = "./ROUT", layer = paste0("result_",i,"_",j), 
           driver = "ESRI Shapefile", overwrite_layer = TRUE)
}


```

